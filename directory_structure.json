{
    "run_demo.py": "#!/usr/bin/env python3\n\"\"\"\nDemo runner for Mental Health Companion.\n\"\"\"\nimport sys\nimport os\n\n# Ensure project root is in Python path\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \".\")))\n\nfrom main_agent import run_agent\nfrom config import Config\n\ndef main():\n    print(\"=\" * 60)\n    print(\"Mental Health First-Step Companion - Demo\")\n    print(\"=\" * 60)\n    print(f\"Mode: {'MOCK (No API)' if Config.MOCK_MODE else 'LIVE (Using Gemini)'}\")\n    print(\"This is NOT a substitute for professional mental health care.\")\n    print(\"=\" * 60)\n    \n    test_cases = [\n        \"I'm feeling really anxious about my exam tomorrow\",\n        \"I'm overwhelmed and don't know what to do\",\n        \"Can you help me calm down?\",\n        \"Tell me about mental health resources\"\n    ]\n    \n    for i, test_input in enumerate(test_cases, 1):\n        print(f\"\\n--- Test Case {i} ---\")\n        print(f\"User: {test_input}\")\n        print(\"Agent processing...\")\n        \n        response = run_agent(test_input)\n        \n        print(f\"\\nAgent Response:\\n{response}\")\n        print(\"-\" * 60)\n    \n    print(\"\\nDemo complete. Run app.py for interactive mode.\")\n\nif __name__ == \"__main__\":\n    # Validate config before running\n    try:\n        Config.validate()\n        main()\n    except ValueError as e:\n        print(f\"Configuration Error: {e}\")\n        print(\"Please create a .env file with GEMINI_API_KEYS or set MOCK_MODE=True\")\n        sys.exit(1)",
    "tools": {
        "tools.py": "\"\"\"\nProvides data retrieval tools for the Worker agent.\n\"\"\"\nimport json\nfrom typing import Dict, List, Optional\n\n# Comprehensive helpline database\nHELPLINES = {\n    \"US\": {\n        \"name\": \"988 Suicide & Crisis Lifeline\",\n        \"number\": \"988\",\n        \"website\": \"https://988lifeline.org\",\n        \"hours\": \"24/7\"\n    },\n    \"UK\": {\n        \"name\": \"NHS 111\",\n        \"number\": \"111\",\n        \"website\": \"https://www.nhs.uk\",\n        \"hours\": \"24/7\"\n    },\n    \"IN\": {\n        \"name\": \"Kiran Mental Health Helpline\",\n        \"number\": \"1800-599-0019\",\n        \"website\": \"https://nimhans.ac.in\",\n        \"hours\": \"24/7\"\n    },\n    \"CA\": {\n        \"name\": \"Crisis Services Canada\",\n        \"number\": \"1-833-456-4566\",\n        \"website\": \"https://www.crisisservicescanada.ca\",\n        \"hours\": \"24/7\"\n    },\n    \"AU\": {\n        \"name\": \"Lifeline Australia\",\n        \"number\": \"13 11 14\",\n        \"website\": \"https://www.lifeline.org.au\",\n        \"hours\": \"24/7\"\n    },\n    \"Global\": {\n        \"name\": \"Befrienders Worldwide\",\n        \"number\": \"Visit befrienders.org\",\n        \"website\": \"https://www.befrienders.org\",\n        \"hours\": \"Varies by country\"\n    }\n}\n\n# Evidence-based grounding techniques\nTECHNIQUES = {\n    \"box_breathing\": {\n        \"name\": \"Box Breathing\",\n        \"steps\": [\n            \"Inhale slowly through your nose for 4 seconds\",\n            \"Hold your breath for 4 seconds\",\n            \"Exhale slowly through your mouth for 4 seconds\",\n            \"Hold empty for 4 seconds\",\n            \"Repeat 4-5 times\"\n        ],\n        \"description\": \"A calming technique used by first responders to regulate breathing\"\n    },\n    \"54321_grounding\": {\n        \"name\": \"5-4-3-2-1 Grounding\",\n        \"steps\": [\n            \"Name 5 things you can SEE around you\",\n            \"Name 4 things you can TOUCH\",\n            \"Name 3 things you can HEAR\",\n            \"Name 2 things you can SMELL\",\n            \"Name 1 thing you can TASTE\"\n        ],\n        \"description\": \"A sensory awareness technique to reconnect with the present moment\"\n    },\n    \"body_scan\": {\n        \"name\": \"Progressive Body Scan\",\n        \"steps\": [\n            \"Close your eyes and take 3 deep breaths\",\n            \"Focus on your toes - tense for 5 seconds, then release\",\n            \"Move to your calves - tense and release\",\n            \"Continue upward: thighs, stomach, hands, arms, shoulders\",\n            \"End with your face - scrunch, then relax\"\n        ],\n        \"description\": \"Progressive muscle relaxation to release physical tension\"\n    },\n    \"mindful_observation\": {\n        \"name\": \"Mindful Observation\",\n        \"steps\": [\n            \"Pick one small object near you\",\n            \"Observe it as if you've never seen it before\",\n            \"Notice its color, texture, shape, weight\",\n            \"Focus all attention on this object for 60 seconds\"\n        ],\n        \"description\": \"Focus attention to interrupt anxious thoughts\"\n    }\n}\n\nclass Tools:\n    @staticmethod\n    def get_helpline(country_code: str = \"Global\") -> Dict:\n        \"\"\"Returns helpline info for a given country code.\"\"\"\n        return HELPLINES.get(country_code.upper(), HELPLINES[\"Global\"])\n    \n    @staticmethod\n    def get_all_country_codes() -> List[str]:\n        \"\"\"Get list of supported country codes.\"\"\"\n        return list(HELPLINES.keys())\n    \n    @staticmethod\n    def get_grounding_technique(technique_name: str) -> Optional[Dict]:\n        \"\"\"Returns the complete technique details.\"\"\"\n        return TECHNIQUES.get(technique_name)\n    \n    @staticmethod\n    def get_all_techniques() -> List[Dict]:\n        \"\"\"Get all available techniques.\"\"\"\n        return [\n            {\n                \"name\": tech[\"name\"],\n                \"key\": key,\n                \"description\": tech[\"description\"]\n            }\n            for key, tech in TECHNIQUES.items()\n        ]\n    \n    @staticmethod\n    def format_technique_steps(technique: Dict) -> str:\n        \"\"\"Format technique steps for user-friendly display.\"\"\"\n        if not technique:\n            return \"Take a deep breath and focus on the present moment.\"\n        \n        steps = f\"**{technique['name']}**\\n{technique['description']}\\n\\nSteps:\\n\"\n        for i, step in enumerate(technique['steps'], 1):\n            steps += f\"{i}. {step}\\n\"\n        return steps",
        "__init__.py": ""
    },
    "config.py": "\"\"\"\nConfiguration management and API key rotation.\n\"\"\"\nimport os\nimport random\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass Config:\n    # Mock mode for testing without API calls\n    MOCK_MODE = os.getenv(\"MOCK_MODE\", \"True\").lower() == \"true\"\n    \n    # Gemini API Keys (comma-separated list for rotation)\n    GEMINI_API_KEYS = os.getenv(\"GEMINI_API_KEYS\", \"\").split(\",\")\n    GEMINI_API_KEYS = [key.strip() for key in GEMINI_API_KEYS if key.strip()]\n    \n    # Default model\n    MODEL_NAME = os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-flash-8b\")\n    \n    # Generation config\n    TEMPERATURE = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n    MAX_OUTPUT_TOKENS = int(os.getenv(\"MAX_OUTPUT_TOKENS\", \"2048\"))\n    \n    @staticmethod\n    def validate():\n        \"\"\"Validate configuration\"\"\"\n        if not Config.MOCK_MODE and not Config.GEMINI_API_KEYS:\n            raise ValueError(\"No GEMINI_API_KEYS found in environment. Set MOCK_MODE=True or add API keys.\")\n    \n    @staticmethod\n    def rotate_gemini_key():\n        \"\"\"Rotate through available API keys\"\"\"\n        if not Config.GEMINI_API_KEYS:\n            raise ValueError(\"No API keys available for rotation\")\n        return random.choice(Config.GEMINI_API_KEYS)",
    "core": {
        "a2a_protocol.py": "\"\"\"\nAgent-to-Agent communication data structures.\n\"\"\"\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Optional, Dict, Any\n\n@dataclass\nclass PlannerOutput:\n    emotion: str\n    risk_level: str\n    action: str\n    instruction: str\n    technique_suggestion: str\n    needs_validation: bool\n\n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n@dataclass\nclass WorkerOutput:\n    draft_response: str\n    tools_used: List[str]\n    technique_applied: Optional[str] = None\n\n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)\n\n@dataclass\nclass EvaluatorOutput:\n    status: str\n    feedback: str\n    final_response: str\n\n    def to_dict(self) -> Dict[str, Any]:\n        return asdict(self)",
        "__init__.py": "",
        "observability.py": "\"\"\"\nEnhanced observability module for logging agent activities.\nSupports console output, log levels, and optional file logging.\n\"\"\"\n\nimport datetime\nimport threading\nfrom typing import Optional, Any, Dict\nimport json\n\nclass Logger:\n    def __init__(self, log_to_file: bool = False, log_file: str = \"agent_logs.txt\"):\n        self.logs = []\n        self.log_to_file = log_to_file\n        self.log_file = log_file\n        self._lock = threading.Lock()  # Thread safety\n        self._setup_file_logging()\n    \n    def _setup_file_logging(self):\n        \"\"\"Initialize log file if enabled.\"\"\"\n        if self.log_to_file:\n            with open(self.log_file, 'w', encoding='utf-8') as f:\n                f.write(f\"=== Mental Health Companion Logs ===\\n\")\n                f.write(f\"Started: {datetime.datetime.now().isoformat()}\\n\\n\")\n    \n    def log(self, agent_name: str, message: str, data: Optional[Any] = None, level: str = \"INFO\"):\n        \"\"\"\n        Log an event with timestamp, agent name, message, and optional data.\n        \n        Args:\n            agent_name: Name of the agent/component logging\n            message: Main log message\n            data: Optional additional data (will be JSON-serialized)\n            level: Log level (INFO, WARNING, ERROR, DEBUG)\n        \"\"\"\n        timestamp = datetime.datetime.now().strftime(\"%H:%M:%S\")\n        \n        # Format log entry\n        entry = f\"[{timestamp}] {level:<5} {agent_name}: {message}\"\n        \n        # Add data if provided\n        if data is not None:\n            try:\n                # Serialize data nicely\n                if isinstance(data, dict):\n                    data_str = json.dumps(data, indent=2)\n                else:\n                    data_str = str(data)\n                entry += f\"\\n{' '*20} Data: {data_str}\"\n            except Exception:\n                entry += f\"\\n{' '*20} Data: {str(data)}\"\n        \n        # Thread-safe logging\n        with self._lock:\n            # Print to console (always)\n            print(entry)\n            \n            # Store in memory\n            self.logs.append(entry)\n            \n            # Write to file if enabled\n            if self.log_to_file:\n                try:\n                    with open(self.log_file, 'a', encoding='utf-8') as f:\n                        f.write(entry + \"\\n\")\n                except Exception as e:\n                    print(f\"[{timestamp}] ERROR Logger: Failed to write to log file: {e}\")\n    \n    def info(self, agent_name: str, message: str, data: Optional[Any] = None):\n        \"\"\"Convenience method for INFO level logs.\"\"\"\n        self.log(agent_name, message, data, level=\"INFO\")\n    \n    def error(self, agent_name: str, message: str, data: Optional[Any] = None):\n        \"\"\"Convenience method for ERROR level logs.\"\"\"\n        self.log(agent_name, message, data, level=\"ERROR\")\n    \n    def warning(self, agent_name: str, message: str, data: Optional[Any] = None):\n        \"\"\"Convenience method for WARNING level logs.\"\"\"\n        self.log(agent_name, message, data, level=\"WARNING\")\n    \n    def debug(self, agent_name: str, message: str, data: Optional[Any] = None):\n        \"\"\"Convenience method for DEBUG level logs.\"\"\"\n        self.log(agent_name, message, data, level=\"DEBUG\")\n    \n    def get_logs(self, last_n: Optional[int] = None) -> str:\n        \"\"\"\n        Get all logs or last N logs as a formatted string.\n        \n        Args:\n            last_n: Number of recent logs to return (None for all)\n        \"\"\"\n        with self._lock:\n            if last_n:\n                logs_to_return = self.logs[-last_n:]\n            else:\n                logs_to_return = self.logs\n            \n            return \"\\n\".join(logs_to_return)\n    \n    def clear(self):\n        \"\"\"Clear all logs from memory.\"\"\"\n        with self._lock:\n            self.logs.clear()\n    \n    def get_stats(self) -> Dict[str, int]:\n        \"\"\"Get logging statistics.\"\"\"\n        with self._lock:\n            return {\n                \"total_logs\": len(self.logs),\n                \"by_level\": {\n                    \"INFO\": sum(1 for log in self.logs if \" INFO \" in log),\n                    \"ERROR\": sum(1 for log in self.logs if \" ERROR \" in log),\n                    \"WARNING\": sum(1 for log in self.logs if \" WARNING \" in log),\n                    \"DEBUG\": sum(1 for log in self.logs if \" DEBUG \" in log)\n                }\n            }\n\n# Singleton instance for global use\nlogger = Logger(log_to_file=False)",
        "context_engineering.py": "\"\"\"\nSystem prompts (personas) for the agents with enhanced safety guidelines.\n\"\"\"\n\nPLANNER_PROMPT = \"\"\"\nYou are an empathetic Mental Health Triage Planner. Your goal is to analyze user input and conversation history to decide the safest, most supportive course of action.\n\nCRITICAL SAFETY RULES:\n- NEVER provide medical diagnosis or treatment advice\n- If user mentions self-harm, suicide, or immediate danger, set risk_level to \"HIGH\" and action to \"emergency_protocol\"\n- For crisis situations, ONLY direct to emergency services - do NOT attempt counseling\n\nANALYZE for:\n- Emotional state: anxiety, sadness, overwhelm, burnout, stress, neutral\n- Risk indicators: self-harm, suicide, violence, medical emergency\n- User needs: grounding, resources, validation, information\n\nOUTPUT FORMAT - JSON only:\n{\n  \"emotion\": \"detected emotional state\",\n  \"risk_level\": \"LOW|MEDIUM|HIGH\",\n  \"action\": \"provide_grounding|provide_resources|emergency_protocol|chat\",\n  \"instruction\": \"Specific, clear instructions for Worker agent\",\n  \"technique_suggestion\": \"box_breathing|54321_grounding|body_scan|none\",\n  \"needs_validation\": true|false\n}\n\nEXAMPLES:\nInput: \"I can't breathe, I'm so stressed about my test.\"\nOutput: {\n  \"emotion\": \"anxiety\",\n  \"risk_level\": \"LOW\",\n  \"action\": \"provide_grounding\",\n  \"instruction\": \"Guide the user through a Box Breathing exercise with calm, simple language.\",\n  \"technique_suggestion\": \"box_breathing\",\n  \"needs_validation\": true\n}\n\nInput: \"I want to end it all.\"\nOutput: {\n  \"emotion\": \"crisis\",\n  \"risk_level\": \"HIGH\",\n  \"action\": \"emergency_protocol\",\n  \"instruction\": \"Provide emergency disclaimer only. Do not give advice or techniques.\",\n  \"technique_suggestion\": \"none\",\n  \"needs_validation\": false\n}\n\"\"\"\n\nWORKER_PROMPT = \"\"\"\nYou are a supportive Mental Health Companion. You provide ONLY non-medical, evidence-based support.\n\nSTRICT LIMITATIONS:\n- NO diagnosis, NO medication advice, NO therapy instructions\n- Use warm, validating, simple language\n- Be concise but thorough\n- Validate emotions before offering techniques\n- Always provide disclaimers where appropriate\n\nRESPONSE STRUCTURE:\n1. Acknowledge their feelings\n2. (If grounding) Explain the technique briefly\n3. Provide step-by-step guidance\n4. Add supportive closing\n5. Include disclaimer: \"This is not a substitute for professional help.\"\n\nDO NOT exceed 200 words unless providing detailed resources.\n\"\"\"\n\nEVALUATOR_PROMPT = \"\"\"\nYou are a Safety Evaluator. Review the Worker's draft response for compliance.\n\nCHECK FOR VIOLATIONS:\n1. MEDICAL ADVICE: Any diagnosis, medication, or treatment suggestions? ‚Üí REJECT\n2. HARMFUL CONTENT: Encourages self-harm or dangerous behavior? ‚Üí REJECT\n3. UNSAFE PROMISES: Guarantees results or makes medical claims? ‚Üí REJECT\n4. TONE: Disrespectful, dismissive, or clinical/jargon-heavy? ‚Üí REJECT\n5. RESOURCE ACCURACY: Fake helplines or unverified sources? ‚Üí REJECT\n6. CRISIS HANDLING: Inadequate emergency response for HIGH risk? ‚Üí REJECT\n\nOUTPUT JSON:\n{\n  \"status\": \"APPROVED|REJECTED\",\n  \"feedback\": \"Specific reason if rejected, or 'Safe and supportive.'\",\n  \"final_response\": \"Original draft if approved, or safe fallback message\"\n}\n\nSAFE FALLBACK: \"I apologize, but I cannot provide that response. Please reach out to a mental health professional or crisis line if you need immediate support.\"\n\"\"\"",
        "gemini_client.py": "\"\"\"\nRobust Gemini API client with key rotation and retry logic.\n\"\"\"\nimport time\nimport json\nfrom typing import Optional, Dict, Any\nimport google.generativeai as genai\nfrom requests.exceptions import RequestException\nfrom core.observability import logger\nfrom config import Config\n\nclass GeminiClient:\n    def __init__(self, system_instruction: str):\n        self.system_instruction = system_instruction\n        self.max_retries = len(Config.GEMINI_API_KEYS) or 1\n        self.retry_delay = 1  # seconds\n        \n    def generate_response(\n        self, \n        prompt: str, \n        json_mode: bool = False\n    ) -> Optional[str]:\n        \"\"\"\n        Generate response with key rotation and retry logic.\n        Returns None if all retries fail.\n        \"\"\"\n        last_exception = None\n        \n        for attempt in range(self.max_retries):\n            try:\n                key = Config.rotate_gemini_key()\n                logger.log(\"GeminiClient\", f\"Using API key {attempt + 1}/{self.max_retries}\")\n                \n                genai.configure(api_key=key)\n                \n                generation_config = {\n                    \"temperature\": Config.TEMPERATURE,\n                    \"top_p\": 0.95,\n                    \"top_k\": 40,\n                    \"max_output_tokens\": Config.MAX_OUTPUT_TOKENS,\n                    \"response_mime_type\": \"application/json\" if json_mode else \"text/plain\",\n                }\n                \n                model = genai.GenerativeModel(\n                    model_name=Config.MODEL_NAME,\n                    generation_config=generation_config,\n                    system_instruction=self.system_instruction\n                )\n                \n                chat_session = model.start_chat(history=[])\n                response = chat_session.send_message(prompt)\n                \n                # Validate response\n                if not response or not response.text:\n                    raise ValueError(\"Empty response from Gemini\")\n                    \n                return response.text.strip()\n                \n            except RequestException as e:\n                logger.log(\"GeminiClient\", f\"Network error (attempt {attempt + 1}): {e}\")\n                last_exception = e\n                time.sleep(self.retry_delay)\n            except Exception as e:\n                logger.log(\"GeminiClient\", f\"API error (attempt {attempt + 1}): {e}\")\n                last_exception = e\n                time.sleep(self.retry_delay)\n        \n        logger.log(\"GeminiClient\", f\"All retries failed. Last error: {last_exception}\")\n        return None\n    \n    def generate_json(self, prompt: str) -> Optional[Dict[str, Any]]:\n        \"\"\"Generate and parse JSON response\"\"\"\n        response_text = self.generate_response(prompt, json_mode=True)\n        if not response_text:\n            return None\n            \n        try:\n            # Clean response\n            if response_text.startswith(\"```json\"):\n                response_text = response_text[7:-3].strip()\n            elif response_text.startswith(\"```\"):\n                response_text = response_text[3:-3].strip()\n                \n            return json.loads(response_text)\n        except json.JSONDecodeError as e:\n            logger.log(\"GeminiClient\", f\"JSON parsing error: {e}\")\n            return None"
    },
    "memory": {
        "session_memory.py": "\"\"\"\nManages short-term conversation history with safety limits.\n\"\"\"\n\nclass SessionMemory:\n    def __init__(self, max_history: int = 10):\n        self.history = []  # List of {\"role\": \"user/assistant\", \"content\": \"...\", \"timestamp\": float}\n        self.max_history = max_history\n    \n    def add_message(self, role: str, content: str):\n        \"\"\"Add message to history with timestamp.\"\"\"\n        import time\n        self.history.append({\n            \"role\": role,\n            \"content\": content,\n            \"timestamp\": time.time()\n        })\n        \n        # Maintain history limit\n        if len(self.history) > self.max_history * 2:  # *2 for user+assistant pairs\n            self.history = self.history[-self.max_history * 2:]\n    \n    def get_history_string(self, last_n: int = 5) -> str:\n        \"\"\"\n        Returns formatted history for LLM context.\n        Includes only last N exchanges to stay within token limits.\n        \"\"\"\n        recent = self.history[-last_n * 2:]  # N exchanges = 2N messages\n        \n        if not recent:\n            return \"No prior conversation.\"\n        \n        formatted = []\n        for msg in recent:\n            role = msg[\"role\"].upper()\n            content = msg[\"content\"]\n            # Truncate very long messages\n            if len(content) > 200:\n                content = content[:200] + \"...\"\n            formatted.append(f\"{role}: {content}\")\n        \n        return \"\\n\".join(formatted)\n    \n    def get_conversation_summary(self) -> str:\n        \"\"\"Get brief summary of conversation flow.\"\"\"\n        if not self.history:\n            return \"New conversation\"\n        \n        user_msgs = [m for m in self.history if m[\"role\"] == \"user\"]\n        return f\"{len(user_msgs)} messages exchanged\"\n    \n    def clear(self):\n        \"\"\"Clear all history.\"\"\"\n        self.history = []\n    \n    def get_stats(self) -> dict:\n        \"\"\"Get conversation statistics.\"\"\"\n        return {\n            \"total_messages\": len(self.history),\n            \"user_messages\": len([m for m in self.history if m[\"role\"] == \"user\"]),\n            \"assistant_messages\": len([m for m in self.history if m[\"role\"] == \"assistant\"])\n        }",
        "__init__.py": ""
    },
    "requirements.txt": "google-generativeai>=0.8.0\ngoogle-genai>=0.3.0\npython-dotenv>=1.0.0\nrequests>=2.31.0",
    "code.py": "import os\nimport json\n\nIGNORE_DIRS = {'.git', '__pycache__', 'venv', 'node_modules', 'cache', 'images','.vscode','drive-mad','code.py'}\n\nIGNORE_EXTENSIONS = {'.png', '.jpg', '.jpeg', '.gif', '.bmp', '.ico', '.exe', '.dll', '.gitignore'}\nALWAYS_TEXT_FILES = {'.md'}\n\ndef read_file(file_path):\n    \"\"\"Try reading a file with UTF-8 first, then UTF-16 as fallback.\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            return f.read()\n    except UnicodeDecodeError:\n        try:\n            with open(file_path, 'r', encoding='utf-16') as f:\n                return f.read()\n        except Exception:\n            return \"<could not read file>\"\n\ndef read_directory(path):\n    result = {}\n    for entry in os.scandir(path):\n        if entry.is_dir():\n            if entry.name in IGNORE_DIRS:\n                continue\n            result[entry.name] = read_directory(entry.path)\n        elif entry.is_file():\n            if any(entry.name.lower().endswith(ext) for ext in IGNORE_EXTENSIONS):\n                continue\n            _, ext = os.path.splitext(entry.name)\n            if ext.lower() in ALWAYS_TEXT_FILES or True:\n                result[entry.name] = read_file(entry.path)\n    return result\n\nroot_dir = '.'  # current directory\n\ndirectory_structure = read_directory(root_dir)\n\nwith open('directory_structure.json', 'w', encoding='utf-8') as f:\n    json.dump(directory_structure, f, ensure_ascii=False, indent=4)\n\nprint(\"Directory structure saved to 'directory_structure.json'\")",
    "__init__.py": "\"\"\"\nMental Health First-Step Companion Package\n\"\"\"\n__version__ = \"1.0.0\"\n__description__ = \"A safe, multi-agent AI system for mental health support\"",
    "agents": {
        "worker.py": "\"\"\"\nWorker Agent: Executes the plan and generates safe, supportive responses.\n\"\"\"\nfrom typing import Dict\nfrom core.context_engineering import WORKER_PROMPT\nfrom core.a2a_protocol import WorkerOutput\nfrom tools.tools import Tools\nfrom core.observability import logger\nfrom core.gemini_client import GeminiClient\n\nclass Worker:\n    def __init__(self):\n        self.client = GeminiClient(WORKER_PROMPT)\n        self.mock_mode = False\n        \n    def work(self, planner_output: Dict) -> Dict:\n        instruction = planner_output.get(\"instruction\", \"\")\n        action = planner_output.get(\"action\", \"\")\n        technique_suggestion = planner_output.get(\"technique_suggestion\", \"none\")\n        \n        logger.log(\"Worker\", \"Executing plan\", \n                  data={\"action\": action, \"technique\": technique_suggestion})\n        \n        # Mock mode\n        if hasattr(self, 'mock_mode') and self.mock_mode:\n            return self._mock_work(planner_output)\n        \n        # Gather context data\n        context_data = \"\"\n        tools_used = []\n        \n        if action == \"provide_grounding\" and technique_suggestion != \"none\":\n            technique = Tools.get_grounding_technique(technique_suggestion)\n            if technique:\n                context_data = Tools.format_technique_steps(technique)\n                tools_used.append(technique_suggestion)\n                \n        elif action == \"provide_resources\":\n            helpline = Tools.get_helpline(\"Global\")\n            context_data = f\"\"\"\n            Available Resources:\n            - {helpline['name']}: {helpline['number']} ({helpline['hours']})\n            - Website: {helpline['website']}\n            \n            For specific countries, provide the country code.\n            \"\"\"\n            tools_used.append(\"helpline_search\")\n            \n        elif action == \"emergency_protocol\":\n            context_data = \"\"\"\n            EMERGENCY PROTOCOL: User may be in crisis.\n            Provide ONLY emergency resources and safety disclaimer.\n            Do NOT provide grounding techniques or advice.\n            \"\"\"\n            tools_used.append(\"emergency_protocol\")\n        \n        # Build prompt\n        prompt = f\"\"\"\n        USER NEED: {instruction}\n        \n        SUPPORT DATA:\n        {context_data}\n        \n        Generate a safe, supportive response following your guidelines.\n        \"\"\"\n        \n        # Generate response\n        draft = self.client.generate_response(prompt)\n        \n        if not draft:\n            # Fallback response\n            draft = \"I apologize, but I'm having trouble generating a response. Please try again, or contact a mental health professional if you need immediate support.\"\n        \n        return WorkerOutput(\n            draft_response=draft,\n            tools_used=tools_used,\n            technique_applied=technique_suggestion if action == \"provide_grounding\" else None\n        ).to_dict()\n    \n    def _mock_work(self, planner_output: Dict) -> Dict:\n        \"\"\"Mock worker for testing.\"\"\"\n        action = planner_output.get(\"action\")\n        instruction = planner_output.get(\"instruction\", \"\")\n        \n        if action == \"emergency_protocol\":\n            draft = \"\"\"\n            ‚ö†Ô∏è **If you're in immediate danger, please contact emergency services now.**\n            \n            In the US: Call or text 988 (Suicide & Crisis Lifeline)\n            In the UK: Call 111 (NHS)\n            Global: Visit befrienders.org\n            \n            You matter. Please reach out for professional support.\n            \"\"\"\n        elif action == \"provide_grounding\":\n            draft = f\"\"\"\n            I hear that you're feeling overwhelmed. Let's try a grounding technique together.\n            \n            **Box Breathing:**\n            1. Breathe in for 4 counts\n            2. Hold for 4 counts  \n            3. Breathe out for 4 counts\n            4. Hold for 4 counts\n            \n            Repeat this 4-5 times. I'm here with you.\n            *This is not a substitute for professional care.*\n            \"\"\"\n        elif action == \"provide_resources\":\n            draft = \"\"\"\n            Here are some trusted mental health resources:\n            \n            **988 Suicide & Crisis Lifeline**\n            Call or text: 988\n            Available 24/7\n            Website: 988lifeline.org\n            \n            **Befrienders Worldwide**\n            Visit: befrienders.org for global resources\n            \n            Remember, seeking help is a sign of strength.\n            \"\"\"\n        else:\n            draft = \"Thank you for sharing. I'm here to listen and support you. What you're feeling is valid.\"\n        \n        return WorkerOutput(\n            draft_response=draft,\n            tools_used=[\"mock_mode\"],\n            technique_applied=\"mock\"\n        ).to_dict()",
        "planner.py": "\"\"\"\nPlanner Agent: Analyzes user input and creates a safe, actionable plan.\n\"\"\"\nimport json\nfrom typing import Dict\nfrom core.context_engineering import PLANNER_PROMPT\nfrom core.a2a_protocol import PlannerOutput\nfrom core.observability import logger\nfrom core.gemini_client import GeminiClient\n\nclass Planner:\n    def __init__(self):\n        self.client = GeminiClient(PLANNER_PROMPT)\n        self.mock_mode = False  # Will be set from config\n        \n    def plan(self, user_input: str, history_str: str) -> Dict:\n        logger.log(\"Planner\", \"Analyzing user input...\", \n                  data={\"input_length\": len(user_input)})\n        \n        # Mock mode for testing\n        if hasattr(self, 'mock_mode') and self.mock_mode:\n            return self._mock_plan(user_input)\n        \n        # Prepare prompt with safety context\n        prompt = f\"\"\"\n        Analyze this conversation and provide a structured plan.\n        \n        CONVERSATION HISTORY:\n        {history_str}\n        \n        CURRENT USER INPUT:\n        {user_input}\n        \n        Remember: Output ONLY valid JSON with no additional text.\n        \"\"\"\n        \n        response_data = self.client.generate_json(prompt)\n        \n        if not response_data:\n            logger.log(\"Planner\", \"Failed to get valid response, using fallback\")\n            return PlannerOutput(\n                emotion=\"unknown\",\n                risk_level=\"LOW\",\n                action=\"chat\",\n                instruction=\"Apologize for technical issues and encourage user to share more.\",\n                technique_suggestion=\"none\",\n                needs_validation=True\n            ).to_dict()\n        \n        # Validate required fields\n        required_fields = [\"emotion\", \"risk_level\", \"action\", \"instruction\"]\n        for field in required_fields:\n            if field not in response_data:\n                response_data[field] = \"unknown\" if field == \"emotion\" else \\\n                                     \"LOW\" if field == \"risk_level\" else \\\n                                     \"chat\" if field == \"action\" else \\\n                                     \"Respond supportively.\"\n        \n        logger.log(\"Planner\", \"Analysis complete\", data=response_data)\n        return response_data\n    \n    def _mock_plan(self, user_input: str) -> Dict:\n        \"\"\"Mock planning for testing without API credits.\"\"\"\n        user_lower = user_input.lower()\n        \n        # Crisis detection (HIGH priority)\n        crisis_keywords = [\"kill myself\", \"end it all\", \"want to die\", \"suicide\", \"self harm\"]\n        if any(word in user_lower for word in crisis_keywords):\n            return PlannerOutput(\n                emotion=\"crisis\",\n                risk_level=\"HIGH\",\n                action=\"emergency_protocol\",\n                instruction=\"Provide emergency disclaimer only. Do not give advice.\",\n                technique_suggestion=\"none\",\n                needs_validation=False\n            ).to_dict()\n        \n        # Anxiety detection\n        anxiety_keywords = [\"anxious\", \"panic\", \"can't breathe\", \"overwhelmed\", \"stressed\"]\n        if any(word in user_lower for word in anxiety_keywords):\n            return PlannerOutput(\n                emotion=\"anxiety\",\n                risk_level=\"LOW\",\n                action=\"provide_grounding\",\n                instruction=\"Guide user through box breathing with calm, simple language.\",\n                technique_suggestion=\"box_breathing\",\n                needs_validation=True\n            ).to_dict()\n        \n        # Sadness detection\n        sadness_keywords = [\"sad\", \"depressed\", \"hopeless\", \"down\"]\n        if any(word in user_lower for word in sadness_keywords):\n            return PlannerOutput(\n                emotion=\"sadness\",\n                risk_level=\"LOW\",\n                action=\"provide_grounding\",\n                instruction=\"Validate feelings and offer 54321 grounding technique.\",\n                technique_suggestion=\"54321_grounding\",\n                needs_validation=True\n            ).to_dict()\n        \n        # Default: supportive chat\n        return PlannerOutput(\n            emotion=\"neutral\",\n            risk_level=\"LOW\",\n            action=\"chat\",\n            instruction=\"Respond with warm, supportive validation of their experience.\",\n            technique_suggestion=\"none\",\n            needs_validation=True\n        ).to_dict()",
        "__init__.py": "",
        "evaluator.py": "\"\"\"\nEvaluator Agent: Safety and quality assurance gatekeeper.\n\"\"\"\nimport re\nfrom typing import Dict\nfrom core.context_engineering import EVALUATOR_PROMPT\nfrom core.a2a_protocol import EvaluatorOutput\nfrom core.observability import logger\nfrom core.gemini_client import GeminiClient\n\nclass Evaluator:\n    def __init__(self):\n        self.client = GeminiClient(EVALUATOR_PROMPT)\n        self.mock_mode = False\n        \n        # Safety filters\n        self.banned_phrases = [\n            r\"\\bdiagnos(e|is)\\b\", r\"\\bmedication\\b\", r\"\\bprescri(be|ption)\\b\",\n            r\"\\btherap(y|ist)\\b.*\\b(recommend|suggest)\", r\"\\bguarantee\\b\", r\"\\bcure\\b\"\n        ]\n        self.crisis_keywords = [\"suicide\", \"self-harm\", \"kill myself\", \"end it all\"]\n        \n    def evaluate(self, worker_output: Dict) -> Dict:\n        draft = worker_output.get(\"draft_response\", \"\")\n        tools_used = worker_output.get(\"tools_used\", [])\n        \n        logger.log(\"Evaluator\", \"Starting safety evaluation\", \n                  data={\"draft_length\": len(draft), \"tools\": tools_used})\n        \n        # Mock mode\n        if hasattr(self, 'mock_mode') and self.mock_mode:\n            return self._mock_evaluate(draft)\n        \n        # Quick safety checks\n        if self._contains_medical_advice(draft):\n            logger.log(\"Evaluator\", \"REJECTED: Medical advice detected\")\n            return EvaluatorOutput(\n                status=\"REJECTED\",\n                feedback=\"Contains medical advice or diagnosis language.\",\n                final_response=self._get_fallback_response()\n            ).to_dict()\n        \n        if self._contains_harmful_content(draft):\n            logger.log(\"Evaluator\", \"REJECTED: Harmful content detected\")\n            return EvaluatorOutput(\n                status=\"REJECTED\",\n                feedback=\"Potentially harmful content detected.\",\n                final_response=self._get_fallback_response()\n            ).to_dict()\n        \n        # Use LLM for deeper evaluation\n        prompt = f\"\"\"\n        DRAFT RESPONSE TO EVALUATE:\n        {draft}\n        \n        TOOLS USED: {', '.join(tools_used)}\n        \n        Provide your safety evaluation JSON.\n        \"\"\"\n        \n        evaluation = self.client.generate_json(prompt)\n        \n        if not evaluation:\n            logger.log(\"Evaluator\", \"Evaluation failed, using fallback\")\n            return EvaluatorOutput(\n                status=\"REJECTED\",\n                feedback=\"Evaluation error.\",\n                final_response=self._get_fallback_response()\n            ).to_dict()\n        \n        # Post-process evaluation\n        if evaluation.get(\"status\") == \"APPROVED\":\n            final_response = draft\n        else:\n            final_response = evaluation.get(\"sanitized_response\", self._get_fallback_response())\n        \n        logger.log(\"Evaluator\", f\"Evaluation result: {evaluation.get('status')}\")\n        \n        return EvaluatorOutput(\n            status=evaluation.get(\"status\", \"REJECTED\"),\n            feedback=evaluation.get(\"feedback\", \"Safety check failed.\"),\n            final_response=final_response\n        ).to_dict()\n    \n    def _contains_medical_advice(self, text: str) -> bool:\n        \"\"\"Check for prohibited medical language.\"\"\"\n        text_lower = text.lower()\n        for pattern in self.banned_phrases:\n            if re.search(pattern, text_lower):\n                return True\n        return False\n    \n    def _contains_harmful_content(self, text: str) -> bool:\n        \"\"\"Check for harmful or dangerous suggestions.\"\"\"\n        return any(keyword in text.lower() for keyword in [\"self-harm\", \"hurt yourself\"])\n    \n    def _get_fallback_response(self) -> str:\n        \"\"\"Safe fallback message for rejected responses.\"\"\"\n        return \"\"\"\n        I apologize, but I cannot provide the requested advice. Please consider reaching out to:\n        \n        **988 Suicide & Crisis Lifeline**: 988 (US)\n        **NHS 111** (UK)\n        **Kiran Helpline**: 1800-599-0019 (India)\n        \n        For professional mental health support, consult a licensed therapist or counselor.\n        \"\"\"\n    \n    def _mock_evaluate(self, draft: str) -> Dict:\n        \"\"\"Mock evaluation for testing.\"\"\"\n        # Simple mock logic\n        if \"diagnosis\" in draft.lower() or \"medication\" in draft.lower():\n            return EvaluatorOutput(\n                status=\"REJECTED\",\n                feedback=\"Mock: Medical advice detected\",\n                final_response=self._get_fallback_response()\n            ).to_dict()\n        \n        return EvaluatorOutput(\n            status=\"APPROVED\",\n            feedback=\"Mock: Safe and supportive\",\n            final_response=draft\n        ).to_dict()"
    },
    "README.md": "# Mental Health First-Step Companion\n\nA production-ready, safety-first AI agent system providing grounding support and mental health resources.\n\n## Features\n\n- **Multi-Agent Architecture**: Planner ‚Üí Worker ‚Üí Evaluator pipeline\n- **Safety-First Design**: Comprehensive content filtering and evaluation\n- **Evidence-Based Techniques**: 5-4-3-2-1 grounding, box breathing, body scan\n- **Global Resources**: Official helplines for US, UK, India, Canada, Australia, and worldwide\n- **Key Rotation**: Automatic fallback across multiple Gemini API keys\n- **Observable**: Detailed logging for debugging and monitoring\n\n## Quick Start\n\n### 1. Setup Environment\n\n```bash\n# Clone repository\ngit clone &lt;your-repo-url&gt;\ncd mental-health-companion\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Configure environment\ncp .env.template .env\n# Edit .env with your Gemini API keys or set MOCK_MODE=True",
    "main_agent.py": "# === main_agent.py (CORRECT VERSION) ===\n\"\"\"\nMain Agent: Orchestrator for the multi-agent pipeline.\n\"\"\"\nfrom agents.planner import Planner\nfrom agents.worker import Worker\nfrom agents.evaluator import Evaluator\nfrom memory.session_memory import SessionMemory\nfrom core.observability import logger\nfrom config import Config\nfrom typing import Dict\n\nclass MainAgent:\n    def __init__(self, mock_mode: bool = None):\n        # Initialize components\n        self.planner = Planner()\n        self.worker = Worker()\n        self.evaluator = Evaluator()\n        self.memory = SessionMemory(max_history=8)\n        \n        # Set mock mode from config or parameter\n        self.mock_mode = mock_mode if mock_mode is not None else Config.MOCK_MODE\n        self.planner.mock_mode = self.mock_mode\n        self.worker.mock_mode = self.mock_mode\n        self.evaluator.mock_mode = self.mock_mode\n        \n        logger.log(\"MainAgent\", f\"Initialized in {'MOCK' if self.mock_mode else 'LIVE'} mode\")\n    \n    def handle_message(self, user_input: str) -> Dict:\n        \"\"\"Process a single user message through the pipeline.\"\"\"\n        logger.log(\"System\", \"Processing new message\", \n                  data={\"input_preview\": user_input[:50] + \"...\"})\n        \n        try:\n            # 1. Update Memory\n            self.memory.add_message(\"user\", user_input)\n            history_str = self.memory.get_history_string()\n            \n            # 2. Planner\n            plan = self.planner.plan(user_input, history_str)\n            \n            # 3. Worker\n            worker_res = self.worker.work(plan)\n            \n            # 4. Evaluator\n            eval_res = self.evaluator.evaluate(worker_res)\n            \n            final_response = eval_res.get(\"final_response\")\n            \n            # 5. Update Memory\n            self.memory.add_message(\"assistant\", final_response)\n            \n            # 6. Compile results\n            return {\n                \"response\": final_response,\n                \"plan\": plan,\n                \"tools_used\": worker_res.get(\"tools_used\", []),\n                \"safety_status\": eval_res.get(\"status\"),\n                \"conversation_stats\": self.memory.get_stats(),\n                \"logs\": logger.get_logs()\n            }\n            \n        except Exception as e:\n            logger.log(\"MainAgent\", f\"Pipeline error: {e}\")\n            error_response = \"I apologize, but I'm experiencing technical difficulties. Please try again later.\"\n            self.memory.add_message(\"assistant\", error_response)\n            \n            return {\n                \"response\": error_response,\n                \"plan\": {\"emotion\": \"error\", \"risk_level\": \"LOW\", \"action\": \"chat\"},\n                \"tools_used\": [],\n                \"safety_status\": \"REJECTED\",\n                \"conversation_stats\": self.memory.get_stats(),\n                \"logs\": logger.get_logs()\n            }\n    \n    def get_conversation_summary(self) -> str:\n        \"\"\"Get summary of current conversation.\"\"\"\n        return self.memory.get_conversation_summary()\n    \n    def clear_memory(self):\n        \"\"\"Clear conversation history.\"\"\"\n        self.memory.clear()\n        logger.log(\"MainAgent\", \"Conversation memory cleared\")\n\ndef run_agent(user_input: str, mock_mode: bool = None) -> str:\n    \"\"\"\n    Convenience function to run agent and get response string.\n    \"\"\"\n    agent = MainAgent(mock_mode=mock_mode)\n    result = agent.handle_message(user_input)\n    return result[\"response\"]",
    ".env": "# Mental Health Companion - Configuration\n# Copy this to .env and fill in your values\n\n# Gemini API Configuration\n# Get keys from: https://makersuite.google.com/app/apikey\n# Format: key1,key2,key3 (comma-separated for rotation)\nGEMINI_API_KEYS=AIzaSyA4shuhq41__bwrZZ4LyhrTQ0igqZrfv9g\n\nGEMINI_MODEL=gemini-1.5-flash-8b\nTEMPERATURE=0.7\nMAX_OUTPUT_TOKENS=2048\n\n# Operation Mode\n# Set to True for testing without API calls\nMOCK_MODE=True\n\n# Logging\nLOG_LEVEL=INFO",
    "app.py": "#!/usr/bin/env python3\n\"\"\"\nInteractive console application for Mental Health Companion.\n\"\"\"\nimport sys\nimport os\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \".\")))\n\nfrom main_agent import MainAgent\nfrom config import Config\n\ndef print_welcome():\n    print(\"=\" * 70)\n    print(\"Mental Health First-Step Companion\")\n    print(\"=\" * 70)\n    print(\"A safe, non-medical support agent for grounding and resources.\")\n    print(\"‚ö†Ô∏è  IMPORTANT: This is NOT a substitute for professional care.\")\n    print(\"‚ö†Ô∏è  If in crisis, contact emergency services immediately.\")\n    print(\"=\" * 70)\n    print(f\"Mode: {'MOCK (Testing)' if Config.MOCK_MODE else 'LIVE (Gemini API)'}\")\n    print(\"Commands: /quit, /clear, /stats, /help\")\n    print(\"=\" * 70)\n\ndef print_help():\n    print(\"\\nAvailable Commands:\")\n    print(\"  /help     - Show this help\")\n    print(\"  /clear    - Clear conversation history\")\n    print(\"  /stats    - Show conversation statistics\")\n    print(\"  /quit     - Exit the application\")\n    print(\"\\nHow to use:\")\n    print(\"  Simply type how you're feeling or what you need help with.\")\n    print(\"  The agent will provide grounding techniques or resources.\\n\")\n\ndef main():\n    try:\n        Config.validate()\n    except ValueError as e:\n        print(f\"Configuration Error: {e}\")\n        print(\"Create a .env file with: GEMINI_API_KEYS=your_key1,your_key2\")\n        print(\"Or set MOCK_MODE=True for testing\")\n        return\n    \n    agent = MainAgent()\n    print_welcome()\n    \n    while True:\n        try:\n            user_input = input(\"\\nYou: \").strip()\n            \n            # Handle commands\n            if user_input.lower() in [\"/quit\", \"/exit\", \"quit\", \"exit\"]:\n                print(\"\\nThank you for using the Mental Health Companion.\")\n                print(\"Take care. üíô\")\n                break\n            \n            elif user_input.lower() == \"/help\":\n                print_help()\n                continue\n            \n            elif user_input.lower() == \"/clear\":\n                agent.clear_memory()\n                print(\"\\nConversation history cleared.\")\n                continue\n            \n            elif user_input.lower() == \"/stats\":\n                stats = agent.get_conversation_summary()\n                print(f\"\\n{stats}\")\n                continue\n            \n            # Skip empty input\n            if not user_input:\n                continue\n            \n            # Process message\n            print(\"\\nAgent: Thinking...\")\n            result = agent.handle_message(user_input)\n            \n            # Display response\n            print(f\"\\n{result['response']}\")\n            \n            # Show debug info in non-mock mode\n            if not Config.MOCK_MODE and result['safety_status'] == 'REJECTED':\n                print(f\"\\n[Debug: Response was modified for safety]\")\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\nExiting... Take care!\")\n            break\n        except Exception as e:\n            print(f\"\\nAn error occurred: {e}\")\n            print(\"Please try again or contact support if this persists.\")\n\nif __name__ == \"__main__\":\n    main()"
}